{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "## 1. Implementing the Sigmoid Activation Function\n",
    "## 2. The \"error\" as the Loss Function\n",
    "## 3. The \"history\" of w1 and w2.\n",
    "\n",
    "## Lab: Homeowork 1 and Github Classroom\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to use interactive plots and are willing to take a risk do the following...\n",
    "\n",
    "## Before we proceed, a STRONG word of caution: this could go wrong.  So if you don't want to take the risk, do NOT do the following.  The quick way around this: Launch jupyter notebook instead of jupyter lab, and use notebook as the backend for matplotlib and not inline:\n",
    "\n",
    "        \n",
    "        \n",
    "```%matplotlib notebook```\n",
    "\n",
    "## Proceed only if you want to have interactive figure in **jupyter lab** -- remember, if you get into trouble, you cannot count on me or the TA to bail you out!\n",
    "\n",
    "### This is what worked for me.  But it has to be the right version of nodejs\n",
    "\n",
    "        conda install -c conda-forge ipympl\n",
    "        conda install -c conda-forge nodejs=10.13\n",
    "        jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "        jupyter lab build\n",
    "\n",
    "### Then use the widget as the backend for matplotlib (instead of inline):\n",
    "\n",
    "```%matplotlib widget```\n",
    "\n",
    "### and for every figure, you need to do\n",
    "\n",
    "```plt.figure()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# All imports\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc95c6094ef4bbba7ea553b5b9984ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Step Function\"\n",
    "\n",
    "step_fun = lambda x: x > 0 \n",
    "# x = np.array([-1, 1])\n",
    "# step_fun(x)\n",
    "x = np.linspace(-10, 10, 100)\n",
    "z = step_fun(x)\n",
    "plt.figure()\n",
    "plt.plot(x, z)\n",
    "plt.ylim(-.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's turn this into a classifier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940d6c9ffb754678a3311032824a4e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Perceptron to implement the NOR operation\n",
    "\n",
    "This version ensures training is only done once, achieving perfect boundary and of course faster speed\n",
    "\n",
    "\n",
    "'''\n",
    "def training(training_data, show_train = False):\n",
    "    # usu. random numbers for weights is not a bad starting point\n",
    "    w = np.random.rand(3)\n",
    "    errors = []\n",
    "\n",
    "    # \"learning rate\"\n",
    "    alfa = 0.2\n",
    "\n",
    "    # use 100 training steps\n",
    "    n = 100\n",
    "    # w.history = []\n",
    "    for i in range(n):\n",
    "        x_train, target = choice(training_data)\n",
    "        y = np.dot(w, x_train)\n",
    "        error = target - step_fun(y)\n",
    "        errors.append(error)\n",
    "        w += alfa * error * x_train\n",
    "\n",
    "    if show_train:\n",
    "        print('weights:', w)\n",
    "        print('Training results:')\n",
    "        for x_train, _ in training_data:\n",
    "            y = np.dot(x_train, w)\n",
    "            print(\"{}: {} -> {}\".format(x_train[:2], y, step_fun(y)))\n",
    "\n",
    "    return w\n",
    "\n",
    "def NOR_perceptron_classifier(x, w_trained=[], show_train=False):    \n",
    "    training_data = [\n",
    "        (np.array([0,0,1]), 1),\n",
    "        (np.array([0,1,1]), 0),\n",
    "        (np.array([1,0,1]), 0),\n",
    "        (np.array([1,1,1]), 0),\n",
    "    ]\n",
    "\n",
    "    if len(w_trained) == 0:\n",
    "        w_trained = training(training_data, show_train = show_train)\n",
    "    \n",
    "    x = np.append(x, 1)\n",
    "    return step_fun(np.dot(x, w_trained)), w_trained\n",
    "\n",
    "x_arr = np.random.rand(1000, 2)\n",
    "\n",
    "z_arr = np.array([])\n",
    "for i, x in enumerate(x_arr):\n",
    "    if i == 0:\n",
    "        z, w_trained = NOR_perceptron_classifier(x)\n",
    "    else:\n",
    "        z, _ = NOR_perceptron_classifier(x, w_trained = w_trained)\n",
    "    z_arr = np.append(z_arr, z)\n",
    "\n",
    "\n",
    "\n",
    "for i, x in enumerate(x_arr):\n",
    "    if z_arr[i]:\n",
    "        plt.plot(x[0], x[1], 'r.')\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], 'b.')\n",
    "\n",
    "x0 = np.array([0, 1])\n",
    "\n",
    "x1 = x0 * (-w_trained[0] / w_trained[1]) - w_trained[2] / w_trained[1]\n",
    "plt.figure()\n",
    "plt.plot(x0, x1, 'g-')\n",
    "plt.axis([-0.1, 1.1, -0.1, 1.1])\n",
    "x_train = [np.array([0, 0, 1, 1]), np.array([0,1,0,1])]\n",
    "plt.plot([0, 1, 1], [1, 0, 1], 'bo', ms = 10)\n",
    "plt.plot([0,], [0,], 'ro', ms = 10);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f07e00344b4807a0c6a6c17bf8e83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Sigmoid function\n",
    "'''\n",
    "plt.figure()\n",
    "y = np.linspace(-10, 10, 100)\n",
    "S = 1 / (1 + np.e ** (-y))\n",
    "plt.plot(y, S)\n",
    "plt.title('The Sigmoid (Logistic) Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Exercise:\n",
    "\n",
    "## Re-implement the perceptron classifier using the sigmoid function, instead of the step function\n",
    "\n",
    "## Preliminary Steps:\n",
    "\n",
    "### 1. Write a sigmoid function\n",
    "### 2. Write a function that calculates the derivative of the sigmoid function (remember it's easier to express it in terms of z, the output -- see lab exercise for Week 5-1 for the tahn function)\n",
    "\n",
    "### 3. Rewrite the classifier function so that it uses the sigmoid rather than the step function as the activation\n",
    "\n",
    "\n",
    "## In addition:\n",
    "\n",
    "## 4. Rewrite the classifier function, such that it has the following call signature\n",
    "\n",
    "     perceptron_classifier(x, N=1000, alfa=0.2, show_train=False, w_trained=[], training_data=[])\n",
    " \n",
    "   ### Obviously, it's NOT OK to have both ```w_trained``` and ```training_data``` as empty list.  You need to \"handle\" that situation in your function defintion.\n",
    "   \n",
    "## 5. After the training is done, plot error and error^2 vs. training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Re-implement perceptron classifier solution:\n",
    "\n",
    "sigmoid function and its derivative\n",
    "\n",
    "'''\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''The logistic function as the activation'''\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    '''derivative of the logistic function'''\n",
    "    return z * (1.0 - z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with random weights, w = [0.45421409 0.15919867 0.531717  ]\n",
      "trained weights, w = [ 3.38588693  3.9193229  -5.52711182]\n",
      "w_history shape (603,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a32a64730f4a43b0cc09e1680fccd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Re-implement perceptron classifier solution:\n",
    "\n",
    "with sigmoid as activation function, and a new call sigature\n",
    "\n",
    "'''\n",
    "\n",
    "def perceptron_classifier(x, N=1000, alfa=0.2, show_train=False, w_trained=[], training_data=[]):    \n",
    "    '''\n",
    "    A general Perceptron with sigmoid as the activation function\n",
    "\n",
    "    Takes in training and testing data.\n",
    "\n",
    "    '''\n",
    "    if len(w_trained) == 0 and len(training_data) == 0:\n",
    "        print('w_trained and trainsing_data cannot both be empty.')\n",
    "        return\n",
    "\n",
    "    errors = []\n",
    "    w_history = np.array([])\n",
    "    if len(w_trained) == 0:\n",
    "        w = np.random.rand(3)\n",
    "        print('Starting with random weights, w = {}'.format(w))\n",
    "        w_history = np.append(w_history, w)\n",
    "        # if you want, you can also see w \"evolves\" over the training stage\n",
    "        for i in range(N):\n",
    "            \n",
    "            x_train, target = choice(training_data)\n",
    "            y = np.dot(w, x_train)\n",
    "            z = sigmoid(y)\n",
    "            error = target - z\n",
    "            errors.append(error)\n",
    "            dP_dw = error * x_train * sigmoid_prime(z)\n",
    "            w += alfa * dP_dw\n",
    "            w_history = np.append(w_history, w)\n",
    "            \n",
    "        if show_train:\n",
    "            print('weights:', w)\n",
    "            print('Training results:')\n",
    "            for x_train, _ in training_data:\n",
    "                y = np.dot(x_train, w)\n",
    "                print(\"{}: {:.4f} -> {}\".format(x_train[:2], y, sigmoid(y)))\n",
    "                \n",
    "    else:\n",
    "        w = w_trained\n",
    "        \n",
    "    x = np.append(x, 1)\n",
    "    return sigmoid(np.dot(x, w)), w, np.array(errors), w_history\n",
    "\n",
    "# NOR\n",
    "# training_data = [\n",
    "#     (np.array([0,0,1]), 1),\n",
    "#     (np.array([0,1,1]), 0),\n",
    "#     (np.array([1,0,1]), 0),\n",
    "#     (np.array([1,1,1]), 0),\n",
    "# ]\n",
    "\n",
    "# OR\n",
    "# training_data = [\n",
    "#     (np.array([0,0,1]), 0),\n",
    "#     (np.array([0,1,1]), 1),\n",
    "#     (np.array([1,0,1]), 1),\n",
    "#     (np.array([1,1,1]), 1),\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# AND\n",
    "training_data = [\n",
    "    (np.array([0,0,1]), 0),\n",
    "    (np.array([0,1,1]), 0),\n",
    "    (np.array([1,0,1]), 0),\n",
    "    (np.array([1,1,1]), 1),\n",
    "]\n",
    "\n",
    "# XOR\n",
    "# training_data = [\n",
    "#     (np.array([0,0,1]), 0),\n",
    "#     (np.array([0,1,1]), 1),\n",
    "#     (np.array([1,0,1]), 1),\n",
    "#     (np.array([1,1,1]), 0),\n",
    "# ]\n",
    "#----------------------------------------------\n",
    "# Now try varying alfa (say bewteen 0.2 and 20!), and N.\n",
    "#----------------------------------------------\n",
    "train_steps = 200\n",
    "alfa = 4\n",
    "x_arr = np.random.rand(1000, 2)\n",
    "\n",
    "z_arr = np.array([])\n",
    "for i, x in enumerate(x_arr):\n",
    "    if i == 0:\n",
    "        z, w_trained, errors, w_history = perceptron_classifier(x, alfa=alfa, N=train_steps, training_data=training_data)\n",
    "        print('trained weights, w = {}'.format(w_trained))\n",
    "        print('w_history shape', w_history.shape)\n",
    "    else:\n",
    "        z = perceptron_classifier(x, w_trained=w_trained)[0]\n",
    "    z_arr = np.append(z_arr, z)\n",
    "    \n",
    "threshold = 0.5\n",
    "plt.figure()\n",
    "for i, x in enumerate(x_arr):\n",
    "    # Here the threshold is set at 0.5 (before, using the step function, it was set at 1)\n",
    "    if z_arr[i] > threshold:\n",
    "        plt.plot(x[0], x[1], 'r.')\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], 'b.')\n",
    "\n",
    "# XOR training set\n",
    "plt.plot([1, 0], [1, 0], 'bo', ms=10) #, label='Trainning Data (Negative)')\n",
    "plt.plot([0, 1], [1, 0], 'ro', ms=10) #, label='Trainning Data (Positive)')\n",
    "# plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45421409 0.15919867 0.531717  ] [ 3.38588693  3.9193229  -5.52711182]\n",
      "(201, 3)\n"
     ]
    }
   ],
   "source": [
    "w_history = w_history.reshape(train_steps+1, -1)\n",
    "print(w_history[0], w_history[-1])\n",
    "print(w_history.shape)\n",
    "\n",
    "w1_history, w2_history = w_history.T[0], w_history.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a257792dc05349669f06954d2a140c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070534a4807f41a1a6748dd3e864369a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Re-implement perceptron classifier solution:\n",
    "\n",
    "error and error^2 as a function of training steps\n",
    "'''\n",
    "plt.figure()\n",
    "plt.plot(range(train_steps), errors)\n",
    "plt.plot(range(train_steps), errors, 'r.')\n",
    "plt.plot(np.zeros(len(errors)), '--')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Training Steps')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(errors**2)\n",
    "plt.plot(errors**2, 'r.')\n",
    "plt.ylabel('Error squared (\"loss\")')\n",
    "plt.xlabel('Training Steps')\n",
    "# plt.ylim(-1e-3, 1e-2 )\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f1fbc108ef43369f4f13fee663f3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = errors ** 2\n",
    "\n",
    "loss_smooth = np.convolve(loss, np.ones((32,))/32, mode='same')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_smooth)\n",
    "plt.plot(loss_smooth, 'r.')\n",
    "plt.ylabel('Error squared (\"loss\")')\n",
    "plt.xlabel('Training Steps')\n",
    "# plt.ylim(-1e-3, 1e-2 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare this error vs. training step vs. the \"binary\" one when we used the step function\n",
    "\n",
    "## Note the error^2 *is* the loss function, it is the negative of the performance function:\n",
    "\n",
    "## $P = -\\frac{1}{2} (d - z)^2$\n",
    "\n",
    "## $\\mathrm{Loss} = \\frac{1}{2} (d - z)^2$\n",
    "\n",
    "## From now on we will use the loss function.\n",
    "\n",
    "## Also Change in notation:\n",
    "\n",
    "## $d \\rightarrow y$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b903dc391ac4d7980bc0504c4cebe73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Note: technically, the weight for the bias should also be shown as it was being trained as well.  \n",
    "But can't show 4D plot easily!\n",
    "\n",
    "Possible improvement: \n",
    "\n",
    "1. Make the points update as training proceeds (see beginning of Week8-1 for dynamic plotting)\n",
    "2. Predict the theoretical shape of P(w0, w1, w2)?  And then project onto P(w1, w2).\n",
    "\n",
    "'''\n",
    "\n",
    "# This import registers the 3D projection, but is otherwise unused.\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "n = 100\n",
    "\n",
    "ax.scatter(w1_history[:-1], w2_history[:-1], loss_smooth, marker='o')\n",
    "ax.plot3D(w1_history[:-1], w2_history[:-1], loss_smooth, 'gray')\n",
    "ax.set_xlabel('w1')\n",
    "ax.set_ylabel('w2')\n",
    "ax.set_zlabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() ()\n"
     ]
    }
   ],
   "source": [
    "print(w_history[:-1].T[0].shape, w_history[:-1].T[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab \n",
    "\n",
    "## 1. Additional thoughts on homework 1 (bounds, maximum iterations)\n",
    "## 2. Github Classroom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Week 6-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

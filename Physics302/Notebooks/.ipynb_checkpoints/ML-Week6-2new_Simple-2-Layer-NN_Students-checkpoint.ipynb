{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "## 1. Many Choices for the Activation Function\n",
    "## 2. Simple Multi-layer Neural Network\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison of the different sigmoids:\n",
    "\n",
    "## (https://en.wikipedia.org/wiki/Activation_function, and there are more)\n",
    "\n",
    "Logistic (a.k.a Soft step)\tActivation logistic:\t$f(x)=\\frac{1}{1+e^{-x}}$, \t$\\,f'(x)=f(x)(1-f(x))$,    (0,1)\t\n",
    "\n",
    "TanH\tActivation tanh:\t$f(x)=\\tanh(x)=\\frac{2}{1+e^{-2x}}-1$,\t$\\,f'(x)=1-f(x)^2$,   (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# All imports\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{:.4f}'.format})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Multi-layer Neural Network\n",
    "\n",
    "(see lecture slides for architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''The logistic function as the sigmoid'''\n",
    "\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_pr(z):\n",
    "    '''derivative of the logistic function'''\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The inner (dot) product and the outer product of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner product:\n",
      " [[11]]\n",
      "outer product:\n",
      " [[3 4]\n",
      " [6 8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.atleast_2d(np.array([1, 2]))\n",
    "b = np.atleast_2d(np.array([3, 4]))\n",
    "\n",
    "inner_prod = np.dot(a, b.T)\n",
    "outer_prod = np.dot(a.T, b)\n",
    "print('inner product:\\n', inner_prod)\n",
    "print('outer product:\\n', outer_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " [[1 2]]\n",
      "\n",
      "a.T:\n",
      " [[1]\n",
      " [2]]\n",
      "\n",
      "b:\n",
      " [[3 4]]\n",
      "\n",
      "b.T transposed:\n",
      " [[3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "print('a:\\n', a)\n",
    "print()\n",
    "print('a.T:\\n', a.T)\n",
    "print()\n",
    "print('b:\\n', b)\n",
    "print()\n",
    "print('b.T transposed:\\n', b.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Excercise -- Write a function simple_nn(X, w1, w2) that performs the forward propagation\n",
    "\n",
    "   - ## X is the input array\n",
    "   - ## w1 are the weights in the first layer (it should be a 2x2 array...and think dot product)\n",
    "   - ## w2 are the weights for the second layer\n",
    "   - ## It should return z, b, A, a\n",
    "   - ## Test it with the input values and weights given in lecture.  And then print out the following:\n",
    "   \n",
    " \n",
    "            w1:\n",
    "             [[0.1000 0.4000]\n",
    "             [0.8000 0.6000]]\n",
    "            w2:\n",
    "             [0.3000 0.9000]\n",
    "            a: [0.7550 0.6800]\n",
    "            A: [0.6803 0.6637]\n",
    "            b: 0.801444986674\n",
    "            output (z) of nn: 0.690283492908 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[0.1000 0.4000]\n",
      " [0.8000 0.6000]]\n",
      "w2:\n",
      " [0.3000 0.9000]\n",
      "a:  [0.7550 0.6800]\n",
      "A:  [0.6803 0.6637]\n",
      "b:  0.8014449866735119\n",
      "output (z) of nn:  0.6902834929076443\n"
     ]
    }
   ],
   "source": [
    "def simple_nn(X, w1new, w2new):\n",
    "    #return z, b, A, a\n",
    "#     a0=(w1[0][0] * X[0]) + ( w1[1][0] * X[1] )\n",
    "#     a1=(w1[0][1] * X[0]) + ( w1[1][1] * X[1] )\n",
    "#     a=[a0, a1]\n",
    "    a = np.dot(X, w1new)\n",
    "    \n",
    "#     A0=sigmoid(a0) \n",
    "#     A1=sigmoid(a1)\n",
    "#     A=[A0, A1]\n",
    "    A = sigmoid(a)\n",
    "    \n",
    "#     b=(A0*w2[0])+(A1*w2[1])\n",
    "    b = np.dot(A, w2new)\n",
    "    \n",
    "    z=sigmoid(b)\n",
    "    \n",
    "    return z, b, A, a\n",
    "    \n",
    "X = np.array([0.35, 0.9])   \n",
    "w1 = np.array([[0.1000, 0.4000], [0.8000, 0.6000]])\n",
    "print('w1:\\n', w1)\n",
    "w2 = np.array([0.3000, 0.9000])\n",
    "print('w2:\\n', w2)\n",
    "\n",
    "z, b, A, a = simple_nn(X, w1, w2)\n",
    "print('a: ', a)\n",
    "print('A: ', A)\n",
    "print('b: ', b)\n",
    "print('output (z) of nn: ', z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9981 0.8353]\n",
      " [0.7423 0.9987]]\n",
      "Transposed:\n",
      " [[0.9981 0.7423]\n",
      " [0.8353 0.9987]]\n"
     ]
    }
   ],
   "source": [
    "#Transpose\n",
    "\n",
    "M = np.random.rand(2, 2)\n",
    "print(M)\n",
    "print('Transposed:\\n', M.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector:\n",
      " [5 8] (2,)\n",
      "Matrix:\n",
      " [[5 8]] (1, 2)\n",
      "Matrix transposed:\n",
      " [[5]\n",
      " [8]] (2, 1)\n",
      "For a 1-D objecte, transpose doesn't change anything: [5 8]\n"
     ]
    }
   ],
   "source": [
    "#np.atleast_2d()\n",
    "aa = np.array([5, 8])\n",
    "AA = np.atleast_2d(aa)\n",
    "print('Vector:\\n', aa, aa.shape)\n",
    "print('Matrix:\\n', AA, AA.shape)\n",
    "print('Matrix transposed:\\n', AA.T, AA.T.shape)\n",
    "print(\"For a 1-D objecte, transpose doesn't change anything:\", aa.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]]\n",
      "[[3]\n",
      " [4]]\n",
      "inner product: \n",
      " [[11]]\n",
      "[[1]\n",
      " [2]]\n",
      "[[3 4]]\n",
      "outer product: \n",
      " [[3 4]\n",
      " [6 8]]\n"
     ]
    }
   ],
   "source": [
    "#the inner (dot) product and the outer product of two vectors\n",
    "AA = np.atleast_2d(np.array([1, 2]))\n",
    "BB = np.atleast_2d(np.array([3, 4]))\n",
    "\n",
    "print(AA)\n",
    "print(BB.T)\n",
    "\n",
    "inner_prod = np.dot(AA, BB.T)\n",
    "outer_prod = np.dot(AA.T, BB)\n",
    "print('inner product: \\n', inner_prod)\n",
    "\n",
    "print(AA.T)\n",
    "print(BB)\n",
    "print('outer product: \\n', outer_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Excercise -- Write a function training_nn(X, y, z, w1, w2) that performs the backward propagation\n",
    "\n",
    "   - ## y is the target\n",
    "   - ## Define delta2 as we talked about in lecture\n",
    "   - ## Also for convenience and clarity, define delta_w2 = delta2 * A.  This is essentially how w2 should be adjusted (with alpha = 1).\n",
    "   - ## Then define delta1 as we talked about in class -- think about the best way to do it \n",
    "   - ## Define a delta_w1.   This is essentially how w1 should be adjusted (with alpha = 1). (*Hint*: Think matrix multiplication.)\n",
    "   - ## Run it and you should get this:\n",
    " \n",
    "\n",
    "            w1:\n",
    "             [[0.1000 0.4000]\n",
    "             [0.8000 0.6000]]\n",
    "            w2:\n",
    "             [0.3000 0.9000]\n",
    "            output (z) of nn: 0.690283492908\n",
    "            \n",
    "            delta2: -0.04068112511233903 ",
    "\n",
    "            delta_w2: [-0.0277 -0.0270] ",
    "\n",
    "            w2: [0.2723 0.8730] ",
    " ",
    "\n",
    "            sigmoid_pr(A): [0.6803 0.6637] ",
    "\n",
    "            delta1: [[-0.0024 -0.0079]] ",
    "\n",
    "            delta_w1: ",
    "\n",
    "            [[-0.0008 -0.0028] ",
    "\n",
    "            [-0.0022 -0.0071]] ",
    "\n",
    "            w1: ",
    "\n",
    "            [[0.0992 0.3972] ",
    "\n",
    "            [0.7978 0.5929]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[0.1000 0.4000]\n",
      " [0.8000 0.6000]]\n",
      "w2:\n",
      " [0.3000 0.9000]\n"
     ]
    }
   ],
   "source": [
    "def training_nn(X, y, z, w1, w2):\n",
    "    alpha=1\n",
    "    error = y-z\n",
    "    delta2 = error * sigmoid_pr(z)\n",
    "    delta_w2 = np.dot(delta2, A)\n",
    "#     print('delta2: ', delta2)\n",
    "#     print('delta_w2: ', delta_w2)\n",
    "\n",
    "    delta1 = np.atleast_2d(delta2 * sigmoid_pr(A) *w2)\n",
    "    X = np.atleast_2d(X)\n",
    "    delta_w1 = np.dot(X.T, delta1)\n",
    "    \n",
    "    w2 += (alpha * delta_w2)\n",
    "#     print('w2: ', w2)\n",
    "#     print('\\n')\n",
    "    \n",
    "#     print('sigmoid_pr(A): ', (A))\n",
    "#     delta1_0 = delta2 * w2[0] * A[0]*(1-A[0])\n",
    "#     delta1_1 = delta2 * w2[1] * A[1]*(1-A[1])\n",
    "#     delta1 = np.atleast_2d(np.array([delta1_0, delta1_1]))\n",
    "    \n",
    "#     print('delta1: ', delta1)\n",
    "#     print('delta_w1: \\n', delta_w1)\n",
    "    w1 += (alpha * delta_w1)\n",
    "    #print('w1: \\n', w1)\n",
    "    \n",
    "    return w2, w1\n",
    "\n",
    "y=0.5\n",
    "X = np.array([0.35, 0.9]) \n",
    "\n",
    "w1 = np.array([[0.1000, 0.4000], [0.8000, 0.6000]])\n",
    "print('w1:\\n', w1)\n",
    "w2 = np.array([0.3000, 0.9000])\n",
    "print('w2:\\n', w2)\n",
    "\n",
    "\n",
    "w2new, w1new = training_nn(X, y, z, w1, w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1_start:\n",
      " [[0.1000 0.4000]\n",
      " [0.8000 0.6000]]\n",
      "w2_start:\n",
      " [0.3000 0.9000]\n",
      "z:  0.6902436813789282\n",
      "z:  0.6820324115901653\n",
      "z:  0.6739604747521266\n",
      "z:  0.666087216261871\n",
      "z:  0.6584281605999255\n",
      "z:  0.6509960787563582\n",
      "z:  0.6438010722624493\n",
      "z:  0.6368507070599206\n",
      "z:  0.63015018513052\n",
      "z:  0.6237025423825647\n",
      "z:  0.6175088623849497\n",
      "z:  0.6115684969534533\n",
      "z:  0.6058792861557863\n",
      "z:  0.6004377718732407\n",
      "z:  0.5952394005383426\n",
      "z:  0.5902787119952019\n",
      "z:  0.5855495125676786\n",
      "z:  0.5810450313587252\n",
      "z:  0.5767580595476445\n",
      "z:  0.5726810730166964\n",
      "z:  0.5688063390467185\n",
      "z:  0.5651260080978882\n",
      "z:  0.5616321918610335\n",
      "z:  0.5583170288499256\n",
      "z:  0.5551727388260017\n",
      "z:  0.552191667321298\n",
      "z:  0.5493663214672256\n",
      "z:  0.5466893982576013\n",
      "z:  0.5441538062829072\n",
      "z:  0.5417526818757601\n",
      "z:  0.5394794005099176\n",
      "z:  0.5373275842002292\n",
      "z:  0.535291105561065\n",
      "z:  0.5333640890972847\n",
      "z:  0.5315409102255151\n",
      "z:  0.5298161924546301\n",
      "z:  0.5281848030928074\n",
      "z:  0.5266418477940651\n",
      "z:  0.5251826642093262\n",
      "z:  0.5238028149652744\n",
      "z:  0.5224980801580176\n",
      "z:  0.5212644495172751\n",
      "z:  0.5200981143699124\n",
      "z:  0.5189954595086502\n",
      "z:  0.5179530550521781\n",
      "z:  0.5169676483662764\n",
      "z:  0.5160361561015026\n",
      "z:  0.5151556563911646\n",
      "z:  0.5143233812433876\n",
      "z:  0.5135367091527929\n",
      "z:  0.5127931579504234\n",
      "z:  0.5120903779048499\n",
      "z:  0.5114261450827026\n",
      "z:  0.510798354973036\n",
      "z:  0.5102050163768208\n",
      "z:  0.5096442455603442\n",
      "z:  0.5091142606693023\n",
      "z:  0.5086133763987918\n",
      "z:  0.5081399989131852\n",
      "z:  0.5076926210089565\n",
      "z:  0.5072698175128334\n",
      "z:  0.5068702409071835\n",
      "z:  0.5064926171742195\n",
      "z:  0.5061357418504377\n",
      "z:  0.505798476282628\n",
      "z:  0.5054797440768193\n",
      "z:  0.505178527731609\n",
      "z:  0.504893865447474\n",
      "z:  0.5046248481038434\n",
      "z:  0.5043706163959408\n",
      "z:  0.5041303581236423\n",
      "z:  0.5039033056248615\n",
      "z:  0.5036887333462479\n",
      "z:  0.5034859555442591\n",
      "z:  0.5032943241099594\n",
      "z:  0.5031132265111723\n",
      "z:  0.5029420838459001\n",
      "z:  0.5027803490011993\n",
      "z:  0.5026275049119688\n",
      "z:  0.5024830629143735\n",
      "z:  0.5023465611888809\n",
      "z:  0.502217563288134\n",
      "z:  0.5020956567451242\n",
      "z:  0.5019804517573535\n",
      "z:  0.5018715799428987\n",
      "z:  0.5017686931644976\n",
      "z:  0.5016714624179793\n",
      "z:  0.5015795767815536\n",
      "z:  0.5014927424226562\n",
      "z:  0.5014106816592212\n",
      "z:  0.5013331320724195\n",
      "z:  0.5012598456680574\n",
      "z:  0.5011905880839824\n",
      "z:  0.5011251378409827\n",
      "z:  0.5010632856348036\n",
      "z:  0.5010048336670346\n",
      "z:  0.500949595012736\n",
      "z:  0.5008973930227967\n",
      "z:  0.5008480607591191\n",
      "z:  0.5008014404608322\n"
     ]
    }
   ],
   "source": [
    "#Training neural net\n",
    "w1 = np.array([[0.1000, 0.4000], [0.8000, 0.6000]])\n",
    "print('w1_start:\\n', w1)\n",
    "w2 = np.array([0.3000, 0.9000])\n",
    "print('w2_start:\\n', w2)\n",
    "\n",
    "training_steps = 100\n",
    "for i in range(training_steps):\n",
    "    #forward propagation\n",
    "    z, b, A, a = simple_nn(X, w1new, w2new)\n",
    "    #backward propagation\n",
    "    w2new, w1new = training_nn(X, y, z, w1, w2)\n",
    "\n",
    "    #printing z value, remember y=0.5\n",
    "    print('z: ', z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Week 6-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "## 1. Many Choices for the Activation Function\n",
    "## 2. Simple Multi-layer Neural Network\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison of the different sigmoids:\n",
    "\n",
    "## (https://en.wikipedia.org/wiki/Activation_function, and there are more)\n",
    "\n",
    "Logistic (a.k.a Soft step)\tActivation logistic:\t$f(x)=\\frac{1}{1+e^{-x}}$, \t$\\,f'(x)=f(x)(1-f(x))$,    (0,1)\t\n",
    "\n",
    "TanH\tActivation tanh:\t$f(x)=\\tanh(x)=\\frac{2}{1+e^{-2x}}-1$,\t$\\,f'(x)=1-f(x)^2$,   (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# All imports\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{:.4f}'.format})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Multi-layer Neural Network\n",
    "\n",
    "(see lecture slides for architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''The logistic function as the sigmoid'''\n",
    "\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_pr(z):\n",
    "    '''derivative of the logistic function'''\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The inner (dot) product and the outer product of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner product:\n",
      " [[11]]\n",
      "outer product:\n",
      " [[3 4]\n",
      " [6 8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.atleast_2d(np.array([1, 2]))\n",
    "b = np.atleast_2d(np.array([3, 4]))\n",
    "\n",
    "inner_prod = np.dot(a, b.T)\n",
    "outer_prod = np.dot(a.T, b)\n",
    "print('inner product:\\n', inner_prod)\n",
    "print('outer product:\\n', outer_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " [[1 2]]\n",
      "\n",
      "a.T:\n",
      " [[1]\n",
      " [2]]\n",
      "\n",
      "b:\n",
      " [[3 4]]\n",
      "\n",
      "b.T transposed:\n",
      " [[3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "print('a:\\n', a)\n",
    "print()\n",
    "print('a.T:\\n', a.T)\n",
    "print()\n",
    "print('b:\\n', b)\n",
    "print()\n",
    "print('b.T transposed:\\n', b.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Excercise -- Write a function simple_nn(X, w1, w2) that performs the forward propagation\n",
    "\n",
    "   - ## X is the input array\n",
    "   - ## w1 are the weights in the first layer (it should be a 2x2 array...and think dot product)\n",
    "   - ## w2 are the weights for the second layer\n",
    "   - ## It should return z, b, A, a\n",
    "   - ## Test it with the input values and weights given in lecture.  And then print out the following:\n",
    "   \n",
    " \n",
    "            w1:\n",
    "             [[0.1000 0.4000]\n",
    "             [0.8000 0.6000]]\n",
    "            w2:\n",
    "             [0.3000 0.9000]\n",
    "            a: [0.7550 0.6800]\n",
    "            A: [0.6803 0.6637]\n",
    "            b: 0.801444986674\n",
    "            output (z) of nn: 0.690283492908 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Excercise -- Write a function training_nn(X, y, w1, w2) that performs the backward propagation\n",
    "\n",
    "   - ## y is the target\n",
    "   - ## Define delta2 as we talked about in lecture\n",
    "   - ## Also for convenience and clarity, define delta_w2 = delta2 * A.  This is essentially how w2 should be adjusted (with alpha = 1).\n",
    "   - ## Then define delta1 as we talked about in class -- think about the best way to do it \n",
    "   - ## Define a delta_w1.   This is essentially how w1 should be adjusted (with alpha = 1). (*Hint*: Think matrix multiplication.)\n",
    "   - ## Run it and you should get this:\n",
    " \n",
    "\n",
    "            w1:\n",
    "             [[0.1000 0.4000]\n",
    "             [0.8000 0.6000]]\n",
    "            w2:\n",
    "             [0.3000 0.9000]\n",
    "            output (z) of nn: 0.690283492908"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Week 6-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
